{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.ops.boxes as box_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to WIDER FACE annotations\n",
    "wider_root = \"/home/hkatti/scratch/datasets/WIDER\"\n",
    "train_annot = os.path.join(wider_root, \"wider_face_split/wider_face_train_bbx_gt.txt\")\n",
    "val_annot = os.path.join(wider_root, \"wider_face_split/wider_face_val_bbx_gt.txt\")\n",
    "wider_images_dir = os.path.join(wider_root, \"WIDER_train/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs:\n",
    "# Configuration for helper funcs (matches paper's parameters)\n",
    "TARGET_SHORT = 600\n",
    "MAX_LONG = 1000\n",
    "FLIP_PROB = 0.5  # 50% chance of horizontal flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading wider annotations\n",
    "def load_wider_annotations(annot_path, images_root):\n",
    "    \"\"\"Properly parses WIDER annotations and constructs full image paths\"\"\"\n",
    "    with open(annot_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    annotations = []\n",
    "    img_paths = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        # Image path relative to WIDER_train/images or WIDER_val/images\n",
    "        rel_path = lines[i].strip()\n",
    "        full_path = os.path.join(images_root, rel_path)\n",
    "        i += 1\n",
    "        \n",
    "        if i >= len(lines):\n",
    "            break\n",
    "            \n",
    "        num_faces = int(lines[i].strip())\n",
    "        i += 1\n",
    "        \n",
    "        # Read bounding boxes and attributes\n",
    "        faces = []\n",
    "        for j in range(num_faces):\n",
    "            if i >= len(lines):\n",
    "                break\n",
    "                \n",
    "            parts = lines[i].strip().split()\n",
    "            i += 1\n",
    "            \n",
    "            # Ensure we have at least the bounding box coordinates\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "                \n",
    "            # Create dictionary with all available attributes\n",
    "            face = {'bbox': [int(parts[0]), int(parts[1]), int(parts[2]), int(parts[3])]}\n",
    "            \n",
    "            # Add attributes if available\n",
    "            if len(parts) > 4:\n",
    "                face['blur'] = int(parts[4]) if len(parts) > 4 else 0\n",
    "                face['expression'] = int(parts[5]) if len(parts) > 5 else 0\n",
    "                face['illumination'] = int(parts[6]) if len(parts) > 6 else 0\n",
    "                face['invalid'] = int(parts[7]) if len(parts) > 7 else 0\n",
    "                face['occlusion'] = int(parts[8]) if len(parts) > 8 else 0\n",
    "                face['pose'] = int(parts[9]) if len(parts) > 9 else 0\n",
    "            \n",
    "            faces.append(face)\n",
    "        \n",
    "        img_paths.append(full_path)\n",
    "        annotations.append(faces)\n",
    "    \n",
    "    return img_paths, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data: 16106 images, 199132 faces\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_imgs, train_anns = load_wider_annotations(train_annot, os.path.join(wider_root, \"WIDER_train/images\"))\n",
    "\n",
    "# Load validation data (to be combined with training)\n",
    "val_imgs, val_anns = load_wider_annotations(val_annot, os.path.join(wider_root, \"WIDER_val/images\"))\n",
    "\n",
    "# Combine datasets as per paper methodology\n",
    "all_images = train_imgs + val_imgs\n",
    "all_annotations = train_anns + val_anns\n",
    "\n",
    "print(f\"Total training data: {len(all_images)} images, {sum(len(a) for a in all_annotations)} faces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing difficulty\n",
    "def compute_difficulty(ann):\n",
    "    \"\"\"\n",
    "    Compute difficulty based on attributes in WIDER FACE annotation\n",
    "    ann: Dictionary with facial attributes\n",
    "    \"\"\"\n",
    "    difficulty = 0\n",
    "    \n",
    "    # Blur\n",
    "    if ann.get('blur', 0) == 1:  # Normal blur\n",
    "        difficulty += 0.5\n",
    "    elif ann.get('blur', 0) == 2:  # Heavy blur\n",
    "        difficulty += 1\n",
    "        \n",
    "    # Expression\n",
    "    if ann.get('expression', 0) == 2:  # Extreme expression\n",
    "        difficulty += 1\n",
    "        \n",
    "    # Illumination\n",
    "    if ann.get('illumination', 0) == 2:  # Extreme illumination\n",
    "        difficulty += 1\n",
    "        \n",
    "    # Occlusion\n",
    "    if ann.get('occlusion', 0) == 1:  # Partial occlusion\n",
    "        difficulty += 0.5\n",
    "    elif ann.get('occlusion', 0) == 2:  # Heavy occlusion\n",
    "        difficulty += 1\n",
    "        \n",
    "    # Pose\n",
    "    if ann.get('pose', 0) == 1:  # Atypical pose\n",
    "        difficulty += 1\n",
    "        \n",
    "    return difficulty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing image improved\n",
    "def resize_image(image, annotations, target_short=600, max_long=1000):\n",
    "    \"\"\"\n",
    "    Resize image and annotations while keeping aspect ratio.\n",
    "    Shorter side is set to target_short, longer side capped at max_long.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        annotations: List of annotation dicts with 'bbox' keys\n",
    "        target_short: Target size for shorter side (default 600)\n",
    "        max_long: Maximum size for longer side (default 1000)\n",
    "    \n",
    "    Returns:\n",
    "        Resized image, updated annotations\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    scale = target_short / min(h, w)\n",
    "    \n",
    "    # Check if scaled longer side exceeds maximum\n",
    "    if max(h, w) * scale > max_long:\n",
    "        scale = max_long / max(h, w)\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "    \n",
    "    # Resize image using OpenCV\n",
    "    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Update annotations\n",
    "    updated_annots = []\n",
    "    for ann in annotations:\n",
    "        # Create copy to avoid modifying original\n",
    "        new_ann = ann.copy()\n",
    "        \n",
    "        # Scale bounding box coordinates [x1, y1, w, h]\n",
    "        x, y, w, h = ann['bbox']\n",
    "        \n",
    "        # Scale coordinates and ensure they're valid\n",
    "        new_x = max(0, x * scale)\n",
    "        new_y = max(0, y * scale)\n",
    "        new_w = min(w * scale, new_w - new_x)\n",
    "        new_h = min(h * scale, new_h - new_y)\n",
    "        \n",
    "        # Skip invalid boxes\n",
    "        if new_w <= 0 or new_h <= 0:\n",
    "            continue\n",
    "            \n",
    "        new_ann['bbox'] = [new_x, new_y, new_w, new_h]\n",
    "        updated_annots.append(new_ann)\n",
    "    \n",
    "    return resized, updated_annots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New horizontal flip function:\n",
    "def horizontal_flip(image, annotations):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: numpy array (H, W, C)\n",
    "        annotations: List of dicts with 'bbox' [x1, y1, w, h]\n",
    "    \n",
    "    Returns:\n",
    "        Flipped image, updated annotations\n",
    "    \"\"\"\n",
    "    flipped_img = cv2.flip(image, 1)  # 1 = horizontal flip\n",
    "    img_w = image.shape[1]\n",
    "    \n",
    "    flipped_annots = []\n",
    "    for ann in annotations:\n",
    "        # Mirror bbox x-coordinate\n",
    "        x, y, w, h = ann['bbox']\n",
    "        new_x = img_w - (x + w)  # x' = image_width - (x + width)\n",
    "        \n",
    "        # Create new annotation with flipped bbox\n",
    "        new_ann = ann.copy()\n",
    "        new_ann['bbox'] = [new_x, y, w, h]\n",
    "        flipped_annots.append(new_ann)\n",
    "    \n",
    "    return flipped_img, flipped_annots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 12 RPN anchors\n",
    "def generate_rpn_anchors():\n",
    "    \"\"\"\n",
    "    Generate 12 anchors per spatial location as per paper:\n",
    "    Sizes: 64x64, 128x128, 256x256, 512x512\n",
    "    Ratios: 1:1, 1:2, 2:1\n",
    "    \"\"\"\n",
    "    base_sizes = [64, 128, 256, 512]  # Anchor sizes\n",
    "    ratios = [1.0, 0.5, 2.0]  # 1:1, 1:2, 2:1\n",
    "    \n",
    "    anchors = []\n",
    "    for size in base_sizes:\n",
    "        area = size ** 2\n",
    "        for ratio in ratios:\n",
    "            # Calculate width/height for each ratio\n",
    "            w = np.sqrt(area / ratio)\n",
    "            h = w * ratio\n",
    "            \n",
    "            # Base anchor at (0,0) - will be shifted during RPN\n",
    "            anchors.append([0, 0, w, h])  # (x_center, y_center, width, height)\n",
    "    \n",
    "    return np.array(anchors, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMS thresholds\n",
    "def apply_nms(proposals, scores, iou_threshold=0.7, top_n=2000):\n",
    "    \"\"\"\n",
    "    Apply NMS to select top 2000 region proposals.\n",
    "    \n",
    "    Args:\n",
    "        proposals: List of region proposals in [x1,y1,x2,y2] format\n",
    "        scores: Confidence scores for each proposal\n",
    "        iou_threshold: IoU threshold for suppression (paper uses 0.7 for RPN)\n",
    "        top_n: Maximum proposals to keep (paper: 2000)\n",
    "    \n",
    "    Returns:\n",
    "        Indices of kept proposals\n",
    "    \"\"\"\n",
    "    # Convert to (x,y,w,h) format for OpenCV NMSBoxes\n",
    "    boxes = [[x1, y1, x2-x1, y2-y1] for x1,y1,x2,y2 in proposals]\n",
    "    \n",
    "    # OpenCV's NMS implementation (optimized)\n",
    "    keep_indices = cv2.dnn.NMSBoxes(\n",
    "        boxes, scores, \n",
    "        score_threshold=0.0,  # Keep all initially\n",
    "        nms_threshold=iou_threshold,\n",
    "        top_k=top_n\n",
    "    )\n",
    "    \n",
    "    return keep_indices.flatten()  # Flatten from 2D to 1D array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROIs for fore and background\n",
    "def sample_rois(proposals, gt_boxes, fg_ratio=0.25):\n",
    "    \"\"\"\n",
    "    Sample RoIs maintaining 1:3 foreground:background ratio.\n",
    "    \n",
    "    Args:\n",
    "        proposals: List of [x1,y1,x2,y2] region proposals (after NMS)\n",
    "        gt_boxes: List of ground truth [x1,y1,x2,y2] boxes\n",
    "        fg_ratio: Desired foreground ratio (1/4 = 1:3)\n",
    "    \n",
    "    Returns:\n",
    "        Sampled indices, labels (1=foreground, 0=background)\n",
    "    \"\"\"\n",
    "    # Compute IoU between proposals and GT boxes\n",
    "    iou_matrix = np.zeros((len(proposals), len(gt_boxes)))\n",
    "    for i, prop in enumerate(proposals):\n",
    "        for j, gt in enumerate(gt_boxes):\n",
    "            iou_matrix[i,j] = calculate_iou(prop, gt)\n",
    "    \n",
    "    # Assign labels: foreground (IoU > 0.5) else background\n",
    "    max_iou = iou_matrix.max(axis=1)\n",
    "    labels = (max_iou > 0.5).astype(np.int32)\n",
    "    \n",
    "    # Split indices\n",
    "    fg_indices = np.where(labels == 1)[0]\n",
    "    bg_indices = np.where(labels == 0)[0]\n",
    "    \n",
    "    # Sample to maintain 1:3 ratio\n",
    "    num_fg = min(len(fg_indices), int(len(proposals)*fg_ratio))\n",
    "    num_bg = min(len(bg_indices), 3*num_fg)\n",
    "    \n",
    "    # Random selection\n",
    "    fg_selected = np.random.choice(fg_indices, num_fg, replace=False)\n",
    "    bg_selected = np.random.choice(bg_indices, num_bg, replace=False)\n",
    "    \n",
    "    # Combine and return\n",
    "    sampled_indices = np.concatenate([fg_selected, bg_selected])\n",
    "    return sampled_indices, labels[sampled_indices]\n",
    "\n",
    "# Helper function for IoU calculation\n",
    "def calculate_iou(box_a, box_b):\n",
    "    \"\"\"Compute Intersection-over-Union for two boxes in [x1,y1,x2,y2] format\"\"\"\n",
    "    x1 = max(box_a[0], box_b[0])\n",
    "    y1 = max(box_a[1], box_b[1])\n",
    "    x2 = min(box_a[2], box_b[2])\n",
    "    y2 = min(box_a[3], box_b[3])\n",
    "    \n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area_a = (box_a[2]-box_a[0]) * (box_a[3]-box_a[1])\n",
    "    area_b = (box_b[2]-box_b[0]) * (box_b[3]-box_b[1])\n",
    "    \n",
    "    return inter_area / (area_a + area_b - inter_area + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanitizing blocks\n",
    "def sanitize_boxes(image, annotations):\n",
    "    \"\"\"\n",
    "    Ensure valid bounding boxes after transformations\n",
    "    Converts between formats if needed and ensures positive width/height\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    valid_annots = []\n",
    "    for ann in annotations:\n",
    "        # Extract bbox coordinates\n",
    "        x, y, w, h = ann['bbox']\n",
    "        \n",
    "        # Ensure positive width and height\n",
    "        if w <= 0 or h <= 0:\n",
    "            continue\n",
    "            \n",
    "        # Clamp coordinates to image boundaries\n",
    "        x = max(0, min(x, width-1))\n",
    "        y = max(0, min(y, height-1))\n",
    "        \n",
    "        # Calculate bottom-right coordinates\n",
    "        x_max = min(x + w, width-1)\n",
    "        y_max = min(y + h, height-1)\n",
    "        \n",
    "        # Recalculate width and height\n",
    "        new_w = x_max - x\n",
    "        new_h = y_max - y\n",
    "        \n",
    "        # Skip invalid boxes\n",
    "        if new_w <= 0 or new_h <= 0:\n",
    "            continue\n",
    "            \n",
    "        # Update annotation\n",
    "        new_ann = ann.copy()\n",
    "        new_ann['bbox'] = [x, y, new_w, new_h]\n",
    "        valid_annots.append(new_ann)\n",
    "        \n",
    "    return valid_annots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/33 [  0%]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/33 [  0%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 1/33 with 498 images to /home/hkatti/scratch/processed_batches/batch_000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 2/33 [  6%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 2/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 3/33 [  9%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 3/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 4/33 [ 12%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 4/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▌        | 5/33 [ 15%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 5/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 6/33 [ 18%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 6/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 7/33 [ 21%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 7/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 8/33 [ 24%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 8/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 9/33 [ 27%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 9/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 10/33 [ 30%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 10/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 11/33 [ 33%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 11/33 with 498 images to /home/hkatti/scratch/processed_batches/batch_010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▋      | 12/33 [ 36%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 12/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 13/33 [ 39%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 13/33 with 497 images to /home/hkatti/scratch/processed_batches/batch_012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 14/33 [ 42%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 14/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  45%|████▌     | 15/33 [ 45%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 15/33 with 497 images to /home/hkatti/scratch/processed_batches/batch_014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|████▊     | 16/33 [ 48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 16/33 with 495 images to /home/hkatti/scratch/processed_batches/batch_015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  52%|█████▏    | 17/33 [ 52%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 17/33 with 497 images to /home/hkatti/scratch/processed_batches/batch_016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  55%|█████▍    | 18/33 [ 55%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 18/33 with 498 images to /home/hkatti/scratch/processed_batches/batch_017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 19/33 [ 58%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 19/33 with 498 images to /home/hkatti/scratch/processed_batches/batch_018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  61%|██████    | 20/33 [ 61%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 20/33 with 497 images to /home/hkatti/scratch/processed_batches/batch_019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  64%|██████▎   | 21/33 [ 64%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 21/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 22/33 [ 67%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 22/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|██████▉   | 23/33 [ 70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 23/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  73%|███████▎  | 24/33 [ 73%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 24/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|███████▌  | 25/33 [ 76%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 25/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  79%|███████▉  | 26/33 [ 79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 26/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  82%|████████▏ | 27/33 [ 82%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 27/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▍ | 28/33 [ 85%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 28/33 with 500 images to /home/hkatti/scratch/processed_batches/batch_027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  88%|████████▊ | 29/33 [ 88%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 29/33 with 498 images to /home/hkatti/scratch/processed_batches/batch_028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  91%|█████████ | 30/33 [ 91%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 30/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  94%|█████████▍| 31/33 [ 94%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 31/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|█████████▋| 32/33 [ 97%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 32/33 with 499 images to /home/hkatti/scratch/processed_batches/batch_031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 33/33 [100%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved batch 33/33 with 106 images to /home/hkatti/scratch/processed_batches/batch_032\n",
      "\n",
      "Processing complete: 16066/16106 images successfully processed (99.8%)\n",
      "Statistics:\n",
      "  - Total images: 16106\n",
      "  - Images loaded: 16106 (100.0%)\n",
      "  - Images with valid annotations: 16075 (99.8% of loaded)\n",
      "  - Images successfully resized: 16066\n",
      "  - Images flipped: 8012 (49.9% of processed)\n",
      "  - Images with errors: 0 (0.0%)\n",
      "  - Final processed images: 16066 (99.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# best - more optimizations: batch processing\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Batch processing configuration\n",
    "BATCH_SIZE = 500\n",
    "SAVE_INTERMEDIATE = True\n",
    "OUTPUT_DIR = \"/home/hkatti/scratch/processed_batches\"\n",
    "SAVE_FORMAT = \"jpg\"  # Options: \"npy\", \"jpg\", \"memmap\"\n",
    "\n",
    "# Create output directory if saving intermediates\n",
    "if SAVE_INTERMEDIATE and not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Initialize statistics dictionary\n",
    "stats = {\n",
    "    \"total_images\": len(all_images),\n",
    "    \"loaded_images\": 0,\n",
    "    \"filtered_images\": 0,\n",
    "    \"resized_images\": 0,\n",
    "    \"flipped_images\": 0,\n",
    "    \"processed_images\": 0,\n",
    "    \"error_images\": 0\n",
    "}\n",
    "\n",
    "# Process in batches to manage memory\n",
    "all_processed_data = []\n",
    "num_batches = (len(all_images) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "# Create overall progress bar for batches\n",
    "batch_progress = tqdm(\n",
    "    range(num_batches),\n",
    "    desc=\"Processing batches\",\n",
    "    unit=\"batch\",\n",
    "    bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{percentage:3.0f}%]\"\n",
    ")\n",
    "\n",
    "for batch_idx in batch_progress:\n",
    "    # Calculate batch range\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(all_images))\n",
    "    \n",
    "    # Get current batch\n",
    "    batch_images = all_images[start_idx:end_idx]\n",
    "    batch_annotations = all_annotations[start_idx:end_idx]\n",
    "    \n",
    "    # Create progress bar for current batch\n",
    "    progress_bar = tqdm(\n",
    "        zip(batch_images, batch_annotations),\n",
    "        total=len(batch_images),\n",
    "        desc=f\"Batch {batch_idx+1}/{num_batches}\",\n",
    "        unit=\"img\",\n",
    "        leave=False,\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{percentage:3.0f}%]\"\n",
    "    )\n",
    "    \n",
    "    # Process current batch\n",
    "    batch_processed_data = []\n",
    "    for img_path, annotations in progress_bar:\n",
    "        try:\n",
    "            # 1. Load image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                stats[\"error_images\"] += 1\n",
    "                continue\n",
    "            stats[\"loaded_images\"] += 1\n",
    "            \n",
    "            # 2. Filter annotations (difficulty <=2 and image has <=1000 faces)\n",
    "            valid_annots = []\n",
    "            if len(annotations) <= 1000:\n",
    "                for ann in annotations:\n",
    "                    if compute_difficulty(ann) <= 2:\n",
    "                        valid_annots.append(ann)\n",
    "            \n",
    "            if not valid_annots:\n",
    "                continue\n",
    "            stats[\"filtered_images\"] += 1\n",
    "            \n",
    "            # 3. Resize image & annotations\n",
    "            try:\n",
    "                resized_img, resized_annots = resize_image(image, valid_annots, \n",
    "                                                         target_short=TARGET_SHORT,\n",
    "                                                         max_long=MAX_LONG)\n",
    "                resized_annots = sanitize_boxes(resized_img, resized_annots)\n",
    "                if not resized_annots:\n",
    "                    continue\n",
    "                stats[\"resized_images\"] += 1\n",
    "            except Exception:\n",
    "                stats[\"error_images\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # 4. Apply horizontal flipping (50% chance)\n",
    "            try:\n",
    "                if random.random() < FLIP_PROB:\n",
    "                    final_img, final_annots = horizontal_flip(resized_img, resized_annots)\n",
    "                    stats[\"flipped_images\"] += 1\n",
    "                else:\n",
    "                    final_img, final_annots = resized_img, resized_annots\n",
    "            except Exception:\n",
    "                stats[\"error_images\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # 5. Prepare for model input\n",
    "            batch_processed_data.append({\n",
    "                'image': final_img.astype(np.float32) / 255.0,\n",
    "                'annotations': final_annots,\n",
    "                'path': img_path\n",
    "            })\n",
    "            stats[\"processed_images\"] += 1\n",
    "            \n",
    "        except Exception:\n",
    "            stats[\"error_images\"] += 1\n",
    "            continue\n",
    "    \n",
    "    # Update main progress bar with overall stats\n",
    "    batch_progress.set_postfix(processed=f\"{stats['processed_images']}/{stats['total_images']}\")\n",
    "    \n",
    "    # Save the processed batch based on the selected format\n",
    "    if SAVE_INTERMEDIATE:\n",
    "        batch_dir = os.path.join(OUTPUT_DIR, f\"batch_{batch_idx:03d}\")\n",
    "        if not os.path.exists(batch_dir):\n",
    "            os.makedirs(batch_dir)\n",
    "        \n",
    "        if SAVE_FORMAT == \"jpg\":\n",
    "            # Save as JPG images with metadata\n",
    "            metadata = []\n",
    "            for i, item in enumerate(batch_processed_data):\n",
    "                # Convert normalized float32 back to uint8 for image saving\n",
    "                img = (item['image'] * 255).astype(np.uint8)\n",
    "                img_file = os.path.join(batch_dir, f\"img_{i:05d}.jpg\")\n",
    "                cv2.imwrite(img_file, img)\n",
    "                \n",
    "                # Store metadata\n",
    "                metadata.append({\n",
    "                    'annotations': item['annotations'],\n",
    "                    'path': item['path'],\n",
    "                    'img_file': f\"img_{i:05d}.jpg\"\n",
    "                })\n",
    "            \n",
    "            # Save metadata separately\n",
    "            meta_file = os.path.join(batch_dir, \"metadata.pkl\")\n",
    "            with open(meta_file, 'wb') as f:\n",
    "                pickle.dump(metadata, f, protocol=4)\n",
    "                \n",
    "        elif SAVE_FORMAT == \"npy\":\n",
    "            # Save each image and annotation separately\n",
    "            for i, item in enumerate(batch_processed_data):\n",
    "                img_file = os.path.join(batch_dir, f\"img_{i:05d}.npy\")\n",
    "                np.save(img_file, item['image'])\n",
    "                \n",
    "                ann_file = os.path.join(batch_dir, f\"ann_{i:05d}.pkl\")\n",
    "                with open(ann_file, 'wb') as f:\n",
    "                    pickle.dump(item['annotations'], f, protocol=4)\n",
    "            \n",
    "            # Save paths\n",
    "            with open(os.path.join(batch_dir, \"paths.txt\"), 'w') as f:\n",
    "                for item in batch_processed_data:\n",
    "                    f.write(f\"{item['path']}\\n\")\n",
    "                    \n",
    "        elif SAVE_FORMAT == \"memmap\":\n",
    "            # Create memmap file for images\n",
    "            n_images = len(batch_processed_data)\n",
    "            if n_images > 0:\n",
    "                img_shape = batch_processed_data[0]['image'].shape\n",
    "                images_file = os.path.join(batch_dir, \"images.dat\")\n",
    "                fp = np.memmap(images_file, dtype='float32', mode='w+', \n",
    "                              shape=(n_images, *img_shape))\n",
    "                \n",
    "                # Write images to memmap file one by one\n",
    "                for i, item in enumerate(batch_processed_data):\n",
    "                    fp[i] = item['image']\n",
    "                \n",
    "                # Flush changes to disk\n",
    "                fp.flush()\n",
    "                del fp\n",
    "                \n",
    "                # Save annotations separately\n",
    "                ann_file = os.path.join(batch_dir, \"annotations.pkl\")\n",
    "                with open(ann_file, 'wb') as f:\n",
    "                    pickle.dump([item['annotations'] for item in batch_processed_data], f, protocol=4)\n",
    "                \n",
    "                # Save paths\n",
    "                with open(os.path.join(batch_dir, \"paths.txt\"), 'w') as f:\n",
    "                    for item in batch_processed_data:\n",
    "                        f.write(f\"{item['path']}\\n\")\n",
    "        \n",
    "        print(f\"\\nSaved batch {batch_idx+1}/{num_batches} with {len(batch_processed_data)} images to {batch_dir}\")\n",
    "    \n",
    "    # Clear batch data to free memory\n",
    "    del batch_processed_data\n",
    "    gc.collect()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nProcessing complete: {stats['processed_images']}/{stats['total_images']} images successfully processed ({stats['processed_images']/stats['total_images']*100:.1f}%)\")\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  - Total images: {stats['total_images']}\")\n",
    "print(f\"  - Images loaded: {stats['loaded_images']} ({stats['loaded_images']/stats['total_images']*100:.1f}%)\")\n",
    "print(f\"  - Images with valid annotations: {stats['filtered_images']} ({stats['filtered_images']/stats['loaded_images']*100:.1f}% of loaded)\")\n",
    "print(f\"  - Images successfully resized: {stats['resized_images']}\")\n",
    "print(f\"  - Images flipped: {stats['flipped_images']} ({stats['flipped_images']/stats['processed_images']*100:.1f}% of processed)\")\n",
    "print(f\"  - Images with errors: {stats['error_images']} ({stats['error_images']/stats['total_images']*100:.1f}%)\")\n",
    "print(f\"  - Final processed images: {stats['processed_images']} ({stats['processed_images']/stats['total_images']*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training:\n",
    "\n",
    "# Memory-efficient dataset\n",
    "class WiderFaceDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_samples=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = []\n",
    "        \n",
    "        # Load metadata from all batches\n",
    "        batch_folders = [f for f in os.listdir(data_dir) if f.startswith(\"batch_\") and os.path.isdir(os.path.join(data_dir, f))]\n",
    "        \n",
    "        for batch_folder in sorted(batch_folders):\n",
    "            batch_path = os.path.join(data_dir, batch_folder)\n",
    "            meta_file = os.path.join(batch_path, \"metadata.pkl\")\n",
    "            \n",
    "            if os.path.exists(meta_file):\n",
    "                with open(meta_file, 'rb') as f:\n",
    "                    batch_metadata = pickle.load(f)\n",
    "                \n",
    "                # Add batch directory to each item\n",
    "                for item in batch_metadata:\n",
    "                    item['batch_dir'] = batch_path\n",
    "                    self.metadata.append(item)\n",
    "                    \n",
    "                    # Stop if we've reached max_samples\n",
    "                    if max_samples is not None and len(self.metadata) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            # Stop if we've reached max_samples\n",
    "            if max_samples is not None and len(self.metadata) >= max_samples:\n",
    "                break\n",
    "                \n",
    "        print(f\"Dataset initialized with {len(self.metadata)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.metadata[idx]\n",
    "        \n",
    "        # Load image on demand\n",
    "        img_file = os.path.join(item['batch_dir'], item['img_file'])\n",
    "        image = cv2.imread(img_file)\n",
    "        # Convert to RGB and normalize\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # HWC -> CHW\n",
    "        \n",
    "        # Extract bounding boxes and convert from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        boxes = []\n",
    "        for ann in item['annotations']:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Ensure positive width and height\n",
    "            if w > 0 and h > 0:\n",
    "                boxes.append([float(x), float(y), float(x + w), float(y + h)])\n",
    "        \n",
    "        # Create tensor from valid boxes\n",
    "        if boxes:\n",
    "            boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.ones(len(boxes), dtype=torch.int64)  # All faces are class 1\n",
    "        else:\n",
    "            # Create dummy box if no valid boxes (will be filtered out during training)\n",
    "            boxes_tensor = torch.tensor([[0.0, 0.0, 1.0, 1.0]], dtype=torch.float32)\n",
    "            labels = torch.zeros(1, dtype=torch.int64)  # Background\n",
    "        \n",
    "        return image, {'boxes': boxes_tensor, 'labels': labels}\n",
    "\n",
    "\n",
    "# Feature concatenation module as described in paper section 3.2\n",
    "class BackboneWithFeatureConcatenation(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(BackboneWithFeatureConcatenation, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.out_channels = 512  # Must define this for Faster R-CNN\n",
    "        \n",
    "        # Define layers for L2 normalization and 1x1 convolution\n",
    "        self.l2_norm = L2Norm()\n",
    "        self.conv1x1 = nn.Conv2d(1280, 512, kernel_size=1)  # 1280 = 256+512+512 (channels from conv3_3, conv4_3, conv5_3)\n",
    "        \n",
    "        # Freeze early VGG layers\n",
    "        for layer in list(self.backbone.children())[:10]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        \n",
    "        # Store intermediate feature maps\n",
    "        intermediate_features = {}\n",
    "        \n",
    "        # Extract features from different VGG layers\n",
    "        for i, layer in enumerate(self.backbone):\n",
    "            x = layer(x)\n",
    "            if i == 16:  # conv3_3 output\n",
    "                intermediate_features['conv3'] = self.l2_norm(x)\n",
    "            elif i == 23:  # conv4_3 output\n",
    "                intermediate_features['conv4'] = self.l2_norm(x)\n",
    "            elif i == 30:  # conv5_3 output\n",
    "                intermediate_features['conv5'] = self.l2_norm(x)\n",
    "        \n",
    "        # Get spatial dimensions of smallest feature map (conv5)\n",
    "        target_size = intermediate_features['conv5'].shape[2:]\n",
    "        \n",
    "        # Resize all feature maps to match conv5's spatial dimensions\n",
    "        resized_conv3 = F.interpolate(intermediate_features['conv3'], \n",
    "                                     size=target_size, \n",
    "                                     mode='bilinear', \n",
    "                                     align_corners=False)\n",
    "        \n",
    "        resized_conv4 = F.interpolate(intermediate_features['conv4'], \n",
    "                                     size=target_size, \n",
    "                                     mode='bilinear', \n",
    "                                     align_corners=False)\n",
    "        \n",
    "        # Concatenate feature maps along channel dimension\n",
    "        combined = torch.cat([resized_conv3, resized_conv4, intermediate_features['conv5']], dim=1)\n",
    "        \n",
    "        # Apply 1x1 convolution to reduce channel dimension\n",
    "        features['0'] = self.conv1x1(combined)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# L2 Normalization layer for feature concatenation\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, scale=20.0):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.scale = scale\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # L2 normalization along channel dimension\n",
    "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()\n",
    "        return self.scale * x / (norm + 1e-10)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, data_loader, num_epochs=10):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer with learning rate from paper\n",
    "    optimizer = optim.SGD(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=0.0001,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (optional)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=3, gamma=0.1\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, targets in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Move data to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += losses.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(data_loader):.4f}\")\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "        }, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_faster_rcnn_model():\n",
    "    # Load pre-trained VGG16\n",
    "    vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "    backbone = vgg16.features\n",
    "    \n",
    "    # Create modified backbone\n",
    "    backbone_with_concat = BackboneWithFeatureConcatenation(backbone)\n",
    "    \n",
    "    # Anchor generator (12 anchors as per paper)\n",
    "    anchor_sizes = ((64,), (128,), (256,), (512,))\n",
    "    # aspect_ratios = ((1.0, 0.5, 2.0),) * len(anchor_sizes)\n",
    "    aspect_ratios = ((1.0, 0.5, 2.0),) \n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=anchor_sizes,\n",
    "        aspect_ratios=aspect_ratios\n",
    "    )\n",
    "    \n",
    "    # ROI aligner\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],  # Now using single combined feature map\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    # Create Faster R-CNN model\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone_with_concat,\n",
    "        num_classes=2,  # Background + Face\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        rpn_pre_nms_top_n_train=6000,\n",
    "        rpn_post_nms_top_n_train=2000,\n",
    "        rpn_nms_thresh=0.7,\n",
    "        box_score_thresh=0.05,\n",
    "        box_nms_thresh=0.3,\n",
    "        box_detections_per_img=100\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with 5000 samples\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   9%|▉         | 474/5000 [00:43<06:23, 11.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 5000/5000 [07:28<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5000/5000 [07:25<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.2380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5000/5000 [07:22<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5000/5000 [07:22<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5000/5000 [07:21<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.1817\n",
      "Training complete and model saved!\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataset without loading all images into memory\n",
    "    dataset = WiderFaceDataset(\"/home/hkatti/scratch/processed_batches\", max_samples=5000)  # Adjust sample count as needed\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: tuple(zip(*batch))  \n",
    "    )\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_faster_rcnn_model()\n",
    "    trained_model = train_model(model, data_loader, num_epochs=5)  # Start with fewer epochs for testing\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(trained_model.state_dict(), \"faster_rcnn_face_detection.pth\")\n",
    "    print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First phase of training done:\n",
    "1. Train on WIDER\n",
    "2. Get the biggest mistakes for hard negative mining \n",
    "3. Below we fine tune on FDDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2596609/423303336.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"faster_rcnn_face_detection.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model loaded successfully!\n",
      "Running inference to find hard negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hard negative mining: 100%|██████████| 5000/5000 [04:25<00:00, 18.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images with hard negatives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning epoch 1/5: 100%|██████████| 5000/5000 [07:20<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.1803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning epoch 2/5: 100%|██████████| 5000/5000 [07:26<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.1736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning epoch 3/5: 100%|██████████| 5000/5000 [07:25<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning epoch 4/5: 100%|██████████| 5000/5000 [07:19<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning epoch 5/5: 100%|██████████| 5000/5000 [07:20<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.1606\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Hard Negative Mining with Pre-trained Model\n",
    "# Load the pre-trained model\n",
    "def load_pretrained_model():\n",
    "    # Create model architecture\n",
    "    model = create_faster_rcnn_model()\n",
    "    \n",
    "    # Load saved weights\n",
    "    model.load_state_dict(torch.load(\"faster_rcnn_face_detection.pth\"))\n",
    "    \n",
    "    # Set to evaluation mode for inference\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Pre-trained model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "# Use the loaded model for hard negative mining\n",
    "def hard_negative_mining(model, data_loader, confidence_threshold=0.8, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Implements hard negative mining as described in the paper:\n",
    "    - Run inference on training data\n",
    "    - Collect regions with confidence > 0.8 but IoU < 0.5 with any ground truth\n",
    "    - These are \"hard negatives\" (confident false positives)\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    hard_negatives = []\n",
    "    \n",
    "    print(\"Running inference to find hard negatives...\")\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Hard negative mining\"):\n",
    "            # Move to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Process each image in the batch\n",
    "            for i, (prediction, target) in enumerate(zip(predictions, targets_on_device)):\n",
    "                # Get boxes with high confidence\n",
    "                high_conf_indices = torch.where(prediction['scores'] > confidence_threshold)[0]\n",
    "                if len(high_conf_indices) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                high_conf_boxes = prediction['boxes'][high_conf_indices]\n",
    "                high_conf_scores = prediction['scores'][high_conf_indices]\n",
    "                \n",
    "                # Get ground truth boxes\n",
    "                gt_boxes = target['boxes']\n",
    "                \n",
    "                # Calculate IoU between predictions and ground truth\n",
    "                hard_neg_indices = []\n",
    "                for j, box in enumerate(high_conf_boxes):\n",
    "                    # Calculate IoU with all ground truth boxes\n",
    "                    ious = box_ops.box_iou(box.unsqueeze(0), gt_boxes)\n",
    "                    max_iou = ious.max().item()\n",
    "                    \n",
    "                    # If max IoU is below threshold, this is a hard negative\n",
    "                    if max_iou < iou_threshold:\n",
    "                        hard_neg_indices.append(high_conf_indices[j])\n",
    "                \n",
    "                # If we found hard negatives, add them to our collection\n",
    "                if hard_neg_indices:\n",
    "                    hard_neg_boxes = prediction['boxes'][hard_neg_indices]\n",
    "                    hard_neg_scores = prediction['scores'][hard_neg_indices]\n",
    "                    \n",
    "                    # Store the image and hard negative regions\n",
    "                    hard_negatives.append({\n",
    "                        'image': images[i].cpu(),\n",
    "                        'hard_neg_boxes': hard_neg_boxes.cpu(),\n",
    "                        'hard_neg_scores': hard_neg_scores.cpu()\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(hard_negatives)} images with hard negatives\")\n",
    "    return hard_negatives\n",
    "\n",
    "# Create a dataset class for hard negatives\n",
    "class HardNegativeDataset(Dataset):\n",
    "    def __init__(self, original_dataset, hard_negatives):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.hard_negatives = hard_negatives\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset) + len(self.hard_negatives)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.original_dataset):\n",
    "            # Return original sample\n",
    "            return self.original_dataset[idx]\n",
    "        else:\n",
    "            # Return hard negative sample\n",
    "            hard_neg_idx = idx - len(self.original_dataset)\n",
    "            hard_neg = self.hard_negatives[hard_neg_idx]\n",
    "            \n",
    "            # Create target with both ground truth and hard negatives\n",
    "            image = hard_neg['image']\n",
    "            \n",
    "            # Create boxes tensor with hard negatives labeled as background (0)\n",
    "            boxes = hard_neg['hard_neg_boxes']\n",
    "            labels = torch.zeros(len(boxes), dtype=torch.int64)  # All hard negatives are background\n",
    "            \n",
    "            return image, {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "# Fine-tune with hard negatives\n",
    "def fine_tune_with_hard_negatives(model, dataset, hard_negatives, num_epochs=5):\n",
    "    # Create combined dataset\n",
    "    combined_dataset = HardNegativeDataset(dataset, hard_negatives)\n",
    "    \n",
    "    # Create data loader\n",
    "    data_loader = DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: tuple(zip(*batch))\n",
    "    )\n",
    "    \n",
    "    # Fine-tune model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer with lower learning rate for fine-tuning\n",
    "    optimizer = optim.SGD(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=0.00005,  # Lower learning rate for fine-tuning\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, (images, targets) in enumerate(tqdm(data_loader, desc=f\"Fine-tuning epoch {epoch+1}/{num_epochs}\")):\n",
    "            try:\n",
    "                # Move data to device\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                # Forward pass\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += losses.item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(data_loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Execute hard negative mining and fine-tuning with pre-trained model\n",
    "model = load_pretrained_model()\n",
    "hard_negatives = hard_negative_mining(model, data_loader)\n",
    "model = fine_tune_with_hard_negatives(model, dataset, hard_negatives)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"faster_rcnn_face_detection_with_hard_negatives.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2845 images with 5171 faces from FDDB\n"
     ]
    }
   ],
   "source": [
    "# 15: FDDB Dataset Preparation\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FDDBDataset(Dataset):\n",
    "    def __init__(self, fddb_dir, fold_id=None, transform=None):\n",
    "        \"\"\"\n",
    "        FDDB Dataset loader\n",
    "        \n",
    "        Args:\n",
    "            fddb_dir: Root directory of FDDB dataset\n",
    "            fold_id: If provided, only load this specific fold (1-10)\n",
    "            transform: Optional transforms to apply\n",
    "        \"\"\"\n",
    "        self.fddb_dir = fddb_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        \n",
    "        # FDDB directory structure\n",
    "        self.images_dir = os.path.join(fddb_dir, \"originalPics\")\n",
    "        \n",
    "        # Load fold files\n",
    "        fold_files = []\n",
    "        if fold_id is not None:\n",
    "            # Load specific fold\n",
    "            fold_file = os.path.join(fddb_dir, \"FDDB-folds\", f\"FDDB-fold-{fold_id:02d}.txt\")\n",
    "            fold_files = [fold_file]\n",
    "        else:\n",
    "            # Load all folds\n",
    "            for i in range(1, 11):\n",
    "                fold_file = os.path.join(fddb_dir, \"FDDB-folds\", f\"FDDB-fold-{i:02d}.txt\")\n",
    "                fold_files.append(fold_file)\n",
    "        \n",
    "        # Load annotations\n",
    "        for fold_file in fold_files:\n",
    "            annotation_file = fold_file.replace(\".txt\", \"-ellipseList.txt\")\n",
    "            self._load_fold(fold_file, annotation_file)\n",
    "            \n",
    "        print(f\"Loaded {len(self.images)} images with {sum(len(a) for a in self.annotations)} faces from FDDB\")\n",
    "    \n",
    "    def _load_fold(self, image_list_file, annotation_file):\n",
    "        \"\"\"Load images and annotations from a specific fold\"\"\"\n",
    "        # Read image list\n",
    "        with open(image_list_file, 'r') as f:\n",
    "            image_paths = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # Read annotations\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            lines = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            # Get image path\n",
    "            img_path = lines[i]\n",
    "            if img_path not in image_paths:\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Get number of faces\n",
    "            i += 1\n",
    "            if i >= len(lines):\n",
    "                break\n",
    "                \n",
    "            num_faces = int(lines[i])\n",
    "            i += 1\n",
    "            \n",
    "            # Read face annotations\n",
    "            faces = []\n",
    "            for j in range(num_faces):\n",
    "                if i >= len(lines):\n",
    "                    break\n",
    "                    \n",
    "                # FDDB annotations are in ellipse format\n",
    "                # major_axis_radius minor_axis_radius angle center_x center_y 1\n",
    "                ellipse = list(map(float, lines[i].split()))\n",
    "                i += 1\n",
    "                \n",
    "                # Convert ellipse to bounding box\n",
    "                major_axis = ellipse[0]\n",
    "                minor_axis = ellipse[1]\n",
    "                angle = ellipse[2]\n",
    "                center_x = ellipse[3]\n",
    "                center_y = ellipse[4]\n",
    "                \n",
    "                # Calculate bounding box (approximation)\n",
    "                w = 2 * major_axis\n",
    "                h = 2 * minor_axis\n",
    "                x = center_x - w/2\n",
    "                y = center_y - h/2\n",
    "                \n",
    "                faces.append({'bbox': [x, y, w, h]})\n",
    "            \n",
    "            # Add image and annotations\n",
    "            full_img_path = os.path.join(self.images_dir, img_path + \".jpg\")\n",
    "            if os.path.exists(full_img_path):\n",
    "                self.images.append(full_img_path)\n",
    "                self.annotations.append(faces)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get annotations\n",
    "        annotations = self.annotations[idx]\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=[ann['bbox'] for ann in annotations])\n",
    "            image = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            \n",
    "            # Update annotations\n",
    "            for i, box in enumerate(boxes):\n",
    "                annotations[i]['bbox'] = box\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = torch.from_numpy(image.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
    "        \n",
    "        # Convert bounding boxes to [x1, y1, x2, y2] format\n",
    "        boxes = []\n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([float(x), float(y), float(x + w), float(y + h)])\n",
    "        \n",
    "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.ones(len(boxes), dtype=torch.int64)  # All faces are class 1\n",
    "        \n",
    "        return image, {'boxes': boxes_tensor, 'labels': labels}\n",
    "\n",
    "# Create FDDB dataset\n",
    "fddb_dir = \"/home/hkatti/scratch/datasets/FDDB\"  # Update with your FDDB dataset path\n",
    "fddb_dataset = FDDBDataset(fddb_dir)\n",
    "\n",
    "# Create data loader for FDDB\n",
    "fddb_loader = DataLoader(\n",
    "    fddb_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2845 images with 5171 faces from FDDB\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Multi-scale Training for FDDB Fine-tuning (without Albumentations)\n",
    "class MultiScaleTransform:\n",
    "    def __init__(self, scales=[480, 600, 750]):\n",
    "        self.scales = scales\n",
    "        \n",
    "    def __call__(self, image, annotations):\n",
    "        # Randomly select a scale\n",
    "        scale = random.choice(self.scales)\n",
    "        \n",
    "        # Calculate the scaling factor\n",
    "        h, w = image.shape[:2]\n",
    "        scale_factor = scale / min(h, w)\n",
    "        \n",
    "        # Cap the longer side if needed (as in the paper)\n",
    "        if max(h, w) * scale_factor > 1000:\n",
    "            scale_factor = 1000 / max(h, w)\n",
    "        \n",
    "        # Calculate new dimensions\n",
    "        new_h = int(h * scale_factor)\n",
    "        new_w = int(w * scale_factor)\n",
    "        \n",
    "        # Resize image\n",
    "        resized_img = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Scale annotations\n",
    "        resized_annotations = []\n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            new_ann = ann.copy()\n",
    "            new_ann['bbox'] = [x * scale_factor, y * scale_factor, \n",
    "                              w * scale_factor, h * scale_factor]\n",
    "            resized_annotations.append(new_ann)\n",
    "        \n",
    "        return resized_img, resized_annotations\n",
    "\n",
    "# Create FDDB dataset with multi-scale augmentation\n",
    "class FDDBMultiScaleDataset(Dataset):\n",
    "    def __init__(self, fddb_dir, scales=[480, 600, 750]):\n",
    "        self.fddb_dataset = FDDBDataset(fddb_dir)\n",
    "        self.multi_scale_transform = MultiScaleTransform(scales)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fddb_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get original image and annotations\n",
    "        img_path = self.fddb_dataset.images[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        annotations = self.fddb_dataset.annotations[idx]\n",
    "        \n",
    "        # Apply multi-scale transform\n",
    "        resized_img, resized_annotations = self.multi_scale_transform(image, annotations)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.from_numpy(resized_img.astype(np.float32) / 255.0).permute(2, 0, 1)\n",
    "        \n",
    "        # Convert bounding boxes to [x1, y1, x2, y2] format\n",
    "        boxes = []\n",
    "        for ann in resized_annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Ensure positive width and height\n",
    "            if w > 0 and h > 0:\n",
    "                boxes.append([float(x), float(y), float(x + w), float(y + h)])\n",
    "        \n",
    "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.ones(len(boxes), dtype=torch.int64)  # All faces are class 1\n",
    "        \n",
    "        return image_tensor, {'boxes': boxes_tensor, 'labels': labels}\n",
    "\n",
    "# Create FDDB dataset with multi-scale augmentation\n",
    "fddb_multi_scale_dataset = FDDBMultiScaleDataset(fddb_dir)\n",
    "\n",
    "# Create data loader for FDDB with multi-scale augmentation\n",
    "fddb_multi_scale_loader = DataLoader(\n",
    "    fddb_multi_scale_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:   7%|▋         | 2848/40000 [04:32<58:21, 10.61it/s, loss=0.237]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  14%|█▍        | 5692/40000 [09:05<49:50, 11.47it/s, loss=0.172]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  21%|██▏       | 8537/40000 [13:35<45:22, 11.56it/s, loss=0.0606]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  28%|██▊       | 11381/40000 [18:06<38:06, 12.52it/s, loss=0.426]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  36%|███▌      | 14228/40000 [22:41<36:30, 11.76it/s, loss=0.49]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  43%|████▎     | 17071/40000 [27:10<33:11, 11.51it/s, loss=0.0736]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  50%|████▉     | 19917/40000 [31:40<33:44,  9.92it/s, loss=0.932] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  57%|█████▋    | 22761/40000 [36:10<26:19, 10.91it/s, loss=0.268] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  64%|██████▍   | 25608/40000 [40:39<23:41, 10.12it/s, loss=0.13]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  71%|███████   | 28453/40000 [45:05<16:02, 12.00it/s, loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  78%|███████▊  | 31296/40000 [49:33<11:19, 12.80it/s, loss=0.0704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  85%|████████▌ | 34142/40000 [54:01<09:21, 10.43it/s, loss=0.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB:  92%|█████████▏| 36986/40000 [58:24<04:16, 11.73it/s, loss=0.396] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB: 100%|█████████▉| 39831/40000 [1:02:52<00:15, 11.06it/s, loss=0.248] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning on FDDB: 100%|██████████| 40000/40000 [1:03:08<00:00, 10.56it/s, loss=0.0802]\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Fine-tuning on FDDB\n",
    "def fine_tune_on_fddb(model, data_loader, num_iterations=40000, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Fine-tune the model on FDDB dataset as described in the paper:\n",
    "    - 40,000 iterations\n",
    "    - Fixed learning rate of 0.001\n",
    "    - Multi-scale training\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer with learning rate from paper\n",
    "    optimizer = optim.SGD(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    epoch = 0\n",
    "    \n",
    "    # Create an infinite data loader by cycling through the dataset\n",
    "    data_iter = iter(data_loader)\n",
    "    \n",
    "    pbar = tqdm(total=num_iterations, desc=\"Fine-tuning on FDDB\")\n",
    "    \n",
    "    while iteration < num_iterations:\n",
    "        try:\n",
    "            # Get next batch\n",
    "            try:\n",
    "                images, targets = next(data_iter)\n",
    "            except StopIteration:\n",
    "                # Restart the iterator when it's exhausted\n",
    "                data_iter = iter(data_loader)\n",
    "                epoch += 1\n",
    "                print(f\"Starting epoch {epoch}\")\n",
    "                images, targets = next(data_iter)\n",
    "            \n",
    "            # Move data to device\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress\n",
    "            iteration += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=losses.item())\n",
    "            \n",
    "            # Save checkpoint every 5000 iterations\n",
    "            if iteration % 5000 == 0:\n",
    "                torch.save({\n",
    "                    'iteration': iteration,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': losses.item(),\n",
    "                }, f\"fddb_checkpoint_iter_{iteration}.pth\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {iteration}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), \"faster_rcnn_face_detection_fddb.pth\")\n",
    "    return model\n",
    "\n",
    "# Fine-tune the model on FDDB\n",
    "model = fine_tune_on_fddb(model, fddb_multi_scale_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Impossible to know which image is saved - name not saved with folder etc. \n",
    "# # Cell 18: FDDB Evaluation \n",
    "# def evaluate_on_fddb(model, fddb_dir, output_dir=\"fddb_results\"):\n",
    "#     \"\"\"\n",
    "#     Evaluate the model on FDDB dataset following the paper's methodology\n",
    "#     \"\"\"\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Create output directory\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Process each fold\n",
    "#     for fold_id in range(1, 11):\n",
    "#         # Create dataset for this fold\n",
    "#         fold_dataset = FDDBDataset(fddb_dir, fold_id=fold_id)\n",
    "#         fold_loader = DataLoader(\n",
    "#             fold_dataset,\n",
    "#             batch_size=1,\n",
    "#             shuffle=False,\n",
    "#             collate_fn=lambda batch: tuple(zip(*batch))\n",
    "#         )\n",
    "        \n",
    "#         # Create output file for this fold\n",
    "#         det_file = os.path.join(output_dir, f\"fold-{fold_id:02d}-detections.txt\")\n",
    "        \n",
    "#         with open(det_file, 'w') as f_det:\n",
    "#             with torch.no_grad():\n",
    "#                 for i, (images, targets) in enumerate(tqdm(fold_loader, desc=f\"Evaluating fold {fold_id}\")):\n",
    "#                     # Get image path directly from the dataset\n",
    "#                     img_path = fold_dataset.images[i]  # Use the batch index directly\n",
    "#                     img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "                    \n",
    "#                     # Move to device\n",
    "#                     images = [img.to(device) for img in images]\n",
    "                    \n",
    "#                     # Multi-scale testing (optional, as in the paper)\n",
    "#                     # Test with 3 scales and average results\n",
    "#                     scales = [480, 600, 750]\n",
    "#                     all_predictions = []\n",
    "                    \n",
    "#                     for scale in scales:\n",
    "#                         # Resize image\n",
    "#                         orig_img = images[0].cpu().permute(1, 2, 0).numpy() * 255\n",
    "#                         h, w = orig_img.shape[:2]\n",
    "#                         scale_factor = scale / min(h, w)\n",
    "#                         if max(h, w) * scale_factor > 1000:\n",
    "#                             scale_factor = 1000 / max(h, w)\n",
    "                        \n",
    "#                         new_h = int(h * scale_factor)\n",
    "#                         new_w = int(w * scale_factor)\n",
    "                        \n",
    "#                         resized_img = cv2.resize(orig_img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "#                         resized_tensor = torch.from_numpy(resized_img.astype(np.float32) / 255.0).permute(2, 0, 1).to(device)\n",
    "                        \n",
    "#                         # Get predictions\n",
    "#                         predictions = model([resized_tensor])\n",
    "                        \n",
    "#                         # Scale back predictions to original size\n",
    "#                         boxes = predictions[0]['boxes'].cpu() / scale_factor\n",
    "#                         scores = predictions[0]['scores'].cpu()\n",
    "                        \n",
    "#                         # Store predictions\n",
    "#                         all_predictions.append((boxes, scores))\n",
    "                    \n",
    "#                     # Combine predictions from different scales\n",
    "#                     all_boxes = torch.cat([p[0] for p in all_predictions])\n",
    "#                     all_scores = torch.cat([p[1] for p in all_predictions])\n",
    "                    \n",
    "#                     # Apply confidence threshold\n",
    "#                     conf_mask = all_scores > 0.8\n",
    "#                     boxes = all_boxes[conf_mask]\n",
    "#                     scores = all_scores[conf_mask]\n",
    "                    \n",
    "#                     # Apply NMS\n",
    "#                     keep_indices = torchvision.ops.nms(boxes, scores, 0.3)\n",
    "#                     boxes = boxes[keep_indices]\n",
    "#                     scores = scores[keep_indices]\n",
    "                    \n",
    "#                     # Limit to top 100 detections\n",
    "#                     if len(scores) > 100:\n",
    "#                         top_indices = torch.argsort(scores, descending=True)[:100]\n",
    "#                         boxes = boxes[top_indices]\n",
    "#                         scores = scores[top_indices]\n",
    "                    \n",
    "#                     # Write detections to file\n",
    "#                     f_det.write(f\"{img_name}\\n\")\n",
    "#                     f_det.write(f\"{len(boxes)}\\n\")\n",
    "                    \n",
    "#                     for i in range(len(boxes)):\n",
    "#                         # Convert to FDDB format (x, y, w, h, score)\n",
    "#                         x1, y1, x2, y2 = boxes[i].tolist()\n",
    "#                         w = x2 - x1\n",
    "#                         h = y2 - y1\n",
    "#                         score = scores[i].item()\n",
    "#                         f_det.write(f\"{x1} {y1} {w} {h} {score}\\n\")\n",
    "    \n",
    "#     print(f\"Evaluation complete. Results saved to {output_dir}\")\n",
    "#     return output_dir\n",
    "\n",
    "# # Evaluate the model on FDDB\n",
    "# results_dir = evaluate_on_fddb(model, fddb_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_fddb(model, fddb_dir, output_dir=\"fddb_results\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on FDDB dataset following the paper's methodology\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each fold\n",
    "    for fold_id in range(1, 11):\n",
    "        # Create dataset for this fold\n",
    "        fold_dataset = FDDBDataset(fddb_dir, fold_id=fold_id)\n",
    "        fold_loader = DataLoader(\n",
    "            fold_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=lambda batch: tuple(zip(*batch))\n",
    "        )\n",
    "        \n",
    "        # Create output file for this fold\n",
    "        det_file = os.path.join(output_dir, f\"fold-{fold_id:02d}-detections.txt\")\n",
    "        \n",
    "        with open(det_file, 'w') as f_det:\n",
    "            with torch.no_grad():\n",
    "                for i, (images, targets) in enumerate(tqdm(fold_loader, desc=f\"Evaluating fold {fold_id}\")):\n",
    "                    # Get image path directly from the dataset\n",
    "                    img_path = fold_dataset.images[i]  # Use the batch index directly\n",
    "                    \n",
    "                    # Extract relative path without extension\n",
    "                    # Remove images_dir prefix and .jpg extension\n",
    "                    images_dir = os.path.join(fddb_dir, \"originalPics\")\n",
    "                    rel_path = img_path.replace(images_dir + os.path.sep, \"\").replace(\".jpg\", \"\")\n",
    "                    \n",
    "                    # Move to device\n",
    "                    images = [img.to(device) for img in images]\n",
    "                    \n",
    "                    # Multi-scale testing (optional, as in the paper)\n",
    "                    # Test with 3 scales and average results\n",
    "                    scales = [480, 600, 750]\n",
    "                    all_predictions = []\n",
    "                    \n",
    "                    for scale in scales:\n",
    "                        # Resize image\n",
    "                        orig_img = images[0].cpu().permute(1, 2, 0).numpy() * 255\n",
    "                        h, w = orig_img.shape[:2]\n",
    "                        scale_factor = scale / min(h, w)\n",
    "                        if max(h, w) * scale_factor > 1000:\n",
    "                            scale_factor = 1000 / max(h, w)\n",
    "                        \n",
    "                        new_h = int(h * scale_factor)\n",
    "                        new_w = int(w * scale_factor)\n",
    "                        \n",
    "                        resized_img = cv2.resize(orig_img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "                        resized_tensor = torch.from_numpy(resized_img.astype(np.float32) / 255.0).permute(2, 0, 1).to(device)\n",
    "                        \n",
    "                        # Get predictions\n",
    "                        predictions = model([resized_tensor])\n",
    "                        \n",
    "                        # Scale back predictions to original size\n",
    "                        boxes = predictions[0]['boxes'].cpu() / scale_factor\n",
    "                        scores = predictions[0]['scores'].cpu()\n",
    "                        \n",
    "                        # Store predictions\n",
    "                        all_predictions.append((boxes, scores))\n",
    "                    \n",
    "                    # Combine predictions from different scales\n",
    "                    all_boxes = torch.cat([p[0] for p in all_predictions])\n",
    "                    all_scores = torch.cat([p[1] for p in all_predictions])\n",
    "                    \n",
    "                    # Apply confidence threshold\n",
    "                    conf_mask = all_scores > 0.8\n",
    "                    boxes = all_boxes[conf_mask]\n",
    "                    scores = all_scores[conf_mask]\n",
    "                    \n",
    "                    # Apply NMS\n",
    "                    keep_indices = torchvision.ops.nms(boxes, scores, 0.3)\n",
    "                    boxes = boxes[keep_indices]\n",
    "                    scores = scores[keep_indices]\n",
    "                    \n",
    "                    # Limit to top 100 detections\n",
    "                    if len(scores) > 100:\n",
    "                        top_indices = torch.argsort(scores, descending=True)[:100]\n",
    "                        boxes = boxes[top_indices]\n",
    "                        scores = scores[top_indices]\n",
    "                    \n",
    "                    # Write detections to file with full relative path\n",
    "                    f_det.write(f\"{rel_path}\\n\")\n",
    "                    f_det.write(f\"{len(boxes)}\\n\")\n",
    "                    \n",
    "                    for i in range(len(boxes)):\n",
    "                        # Convert to FDDB format (x, y, w, h, score)\n",
    "                        x1, y1, x2, y2 = boxes[i].tolist()\n",
    "                        w = x2 - x1\n",
    "                        h = y2 - y1\n",
    "                        score = scores[i].item()\n",
    "                        f_det.write(f\"{x1} {y1} {w} {h} {score}\\n\")\n",
    "    \n",
    "    print(f\"Evaluation complete. Results saved to {output_dir}\")\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 290 images with 515 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1: 100%|██████████| 290/290 [00:40<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 285 images with 519 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 2: 100%|██████████| 285/285 [00:39<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 274 images with 517 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 3: 100%|██████████| 274/274 [00:37<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 302 images with 517 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 4: 100%|██████████| 302/302 [00:42<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 298 images with 514 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 5: 100%|██████████| 298/298 [00:40<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 302 images with 518 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 6: 100%|██████████| 302/302 [00:42<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 279 images with 518 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 7: 100%|██████████| 279/279 [00:38<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 276 images with 518 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 8: 100%|██████████| 276/276 [00:38<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 259 images with 514 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 9: 100%|██████████| 259/259 [00:36<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 280 images with 521 faces from FDDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fold 10: 100%|██████████| 280/280 [00:39<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to fddb_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on FDDB\n",
    "results_dir = evaluate_on_fddb(model, fddb_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
